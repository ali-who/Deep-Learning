{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Gradient Descent\n",
    "\n",
    "Okay, now we know how to update our weights:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "You've seen how to implement that for a single update, but how do we translate that code to calculate many weight updates so our network will learn?\n",
    "\n",
    "As an example, I'm going to have you use gradient descent to train a network on graduate school admissions data found [here](http://www.ats.ucla.edu/stat/data/binary.csv). This dataset has three input features: GRE score, GPA, and the rank of the undergraduate school (numbered 1 through 4). Institutions with rank 1 have the highest prestige, those with rank 4 have the lowest.\n",
    "\n",
    "#### Programming exercise:\n",
    "\n",
    "\n",
    "Below, you'll implement gradient descent and train the network on the admissions data. Your goal here is to train the network until you reach a minimum in the mean square error (MSE) on the training set. You need to implement:\n",
    "\n",
    "The network output: output.\n",
    "The output error: error.\n",
    "The error term: error_term.\n",
    "Update the weight step: del_w +=.\n",
    "Update the weights: weights +=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "edited": false,
    "gradable": true,
    "grader_id": "kkpzet649mc",
    "udacity_user_query": ""
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def update_weights(weights, features, targets, learnrate):\n",
    "  \"\"\"\n",
    "  Complete a single epoch of gradient descent and return updated weights\n",
    "  \"\"\"\n",
    "  del_w = np.zeros(weights.shape)\n",
    "  # Loop through all records, x is the input, y is the target\n",
    "  for x, y in zip(features.values, targets):\n",
    "      # Calculate the output of f(h) by passing h (the dot product\n",
    "      # of x and weights) into the activation function (sigmoid).\n",
    "    output = sigmoid(np.dot(x, weights))\n",
    "\n",
    "      # Calculate the error by subtracting the network output\n",
    "      # from the target (y).\n",
    "    error = y - output\n",
    "\n",
    "      # Calculate the error term by multiplying the error by the\n",
    "      # gradient.\n",
    "      # Recall that the gradient of the sigmoid f(h) is\n",
    "      # f(h)*(1âˆ’f(h)) so you do not need to call any additional\n",
    "      # functions and can simply apply this formula to the output and\n",
    "      # error you already calculated.\n",
    "    error_term = error  *output*  (1 - output)\n",
    "\n",
    "      # Update the weight step by multiplying the error term by\n",
    "      # the input (x) and adding this to the current weight step.\n",
    "    del_w += error_term * x\n",
    "\n",
    "      # Update the weights by adding the learning rate times the\n",
    "      # change in weights divided by the number of records.\n",
    "    n_records = features.shape[0]\n",
    "    weights += learnrate * del_w / n_records\n",
    "  \n",
    "  return weights\n",
    "\n",
    "def gradient_descent(features, targets, epochs=1000, learnrate=0.5):\n",
    "    \"\"\"\n",
    "    Perform the complete gradient descent process on a given dataset\n",
    "    \"\"\"\n",
    "    # Use to same seed to make debugging easier\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Initialize loss and weights\n",
    "    last_loss = None\n",
    "    n_features = features.shape[1]\n",
    "    weights = np.random.normal(scale=1/n_features**.5, size=n_features)\n",
    "\n",
    "    # Repeatedly update the weights based on the number of epochs\n",
    "    for e in range(epochs):\n",
    "        weights = update_weights(weights, features, targets, learnrate)\n",
    "\n",
    "        # Printing out the MSE on the training set every 10 epochs.\n",
    "        # Initially this will print the same loss every time.\n",
    "        # The MSE should decrease with each printout\n",
    "        if e % (epochs / 10) == 0:\n",
    "            out = sigmoid(np.dot(features, weights))\n",
    "            loss = np.mean((out - targets) ** 2)\n",
    "            if last_loss and last_loss < loss:\n",
    "                print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "            else:\n",
    "                print(\"Train loss: \", loss)\n",
    "            last_loss = loss\n",
    "            \n",
    "    return weights\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "weights = gradient_descent(features, targets)\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "grader_mode": "",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "showGradeBtn": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
